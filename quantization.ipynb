{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_model_optimization as tfmot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_train_function.<locals>.train_function at 0x000002EC1C3C6C18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 370ms/step - loss: 16.1181 - accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "input_shape = [20]\n",
    "x_train = np.random.randn(1, 20).astype(np.float32)\n",
    "y_train = tf.keras.utils.to_categorical(np.random.randn(1), num_classes=20)\n",
    "\n",
    "def setup_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(20, input_shape=input_shape),\n",
    "        tf.keras.layers.Flatten()\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def setup_pretrained_weights():\n",
    "    model = setup_model()\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.categorical_crossentropy,\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    model.fit(x_train, y_train)\n",
    "    pretrained_weights = './result/model_without_quantization.tf'\n",
    "    model.save_weights(pretrained_weights)\n",
    "    return pretrained_weights\n",
    "\n",
    "def setup_pretrained_model():\n",
    "    model = setup_model()\n",
    "    pretrained_weights = setup_pretrained_weights()\n",
    "    model.load_weights(pretrained_weights)\n",
    "    return model\n",
    "\n",
    "setup_model()\n",
    "pretrained_weights = setup_pretrained_weights()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 量化整个模型\n",
    "\n",
    "1. 尝试量化某些对模型性能影响不大的神经层\n",
    "2. 与从头开始训练相比，使用量化感知训练进行微调的效果一般更好 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "quantize_layer (QuantizeLaye (None, 20)                3         \n",
      "_________________________________________________________________\n",
      "quant_dense_10 (QuantizeWrap (None, 20)                425       \n",
      "_________________________________________________________________\n",
      "quant_flatten_10 (QuantizeWr (None, 20)                1         \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 420\n",
      "Non-trainable params: 9\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = setup_model()\n",
    "base_model.load_weights(pretrained_weights) # optional but recommended for model accuracy\n",
    "\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_model(base_model)\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 量化某些层\n",
    "\n",
    "1. 与从头开始训练相比，使用量化感知训练进行微调效果一般更好\n",
    "2. 尝试量化后面的层而不是前面的层\n",
    "3. 避免量化关键层，如注意力机制层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "quantize_layer_1 (QuantizeLa (None, 20)                3         \n",
      "_________________________________________________________________\n",
      "quant_dense_11 (QuantizeWrap (None, 20)                425       \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 20)                0         \n",
      "=================================================================\n",
      "Total params: 428\n",
      "Trainable params: 420\n",
      "Non-trainable params: 8\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 仅量化Dense层\n",
    "# Create a base model\n",
    "base_model = setup_model()\n",
    "base_model.load_weights(pretrained_weights) # optional but recommended for model accuracy\n",
    "\n",
    "# Helper function uses `quantize_annotate_layer` to annotate that only the \n",
    "# Dense layers should be quantized.\n",
    "def apply_quantization_to_dense(layer):\n",
    "  if isinstance(layer, tf.keras.layers.Dense):\n",
    "    return tfmot.quantization.keras.quantize_annotate_layer(layer)\n",
    "  return layer\n",
    "\n",
    "# Use `tf.keras.models.clone_model` to apply `apply_quantization_to_dense` \n",
    "# to the layers of the model.\n",
    "annotated_model = tf.keras.models.clone_model(\n",
    "    base_model,\n",
    "    clone_function=apply_quantization_to_dense,\n",
    ")\n",
    "\n",
    "# Now that the Dense layers are annotated,\n",
    "# `quantize_apply` actually makes the model quantization aware.\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_apply(annotated_model)\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建部署模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_train_function.<locals>.train_function at 0x000002EC1C326558> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "1/1 [==============================] - 0s 378ms/step - loss: 16.1181 - accuracy: 0.0000e+00\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-0.bias\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.kernel\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-0.bias\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as dense_14_layer_call_fn, dense_14_layer_call_and_return_conditional_losses, flatten_14_layer_call_fn, flatten_14_layer_call_and_return_conditional_losses, dense_14_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as dense_14_layer_call_fn, dense_14_layer_call_and_return_conditional_losses, flatten_14_layer_call_fn, flatten_14_layer_call_and_return_conditional_losses, dense_14_layer_call_fn while saving (showing 5 of 10). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\wendy\\AppData\\Local\\Temp\\tmp67m64bw_\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\wendy\\AppData\\Local\\Temp\\tmp67m64bw_\\assets\n"
     ]
    }
   ],
   "source": [
    "base_model = setup_pretrained_model()\n",
    "quant_aware_model = tfmot.quantization.keras.quantize_model(base_model)\n",
    "\n",
    "# Typically you train the model here.\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(quant_aware_model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "\n",
    "quantized_tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 量化自定义层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "LastValueQuantizer = tfmot.quantization.keras.quantizers.LastValueQuantizer\n",
    "MovingAverageQuantizer = tfmot.quantization.keras.quantizers.MovingAverageQuantizer\n",
    "\n",
    "class DefaultDenseQuantizeConfig(tfmot.quantization.keras.QuantizeConfig):\n",
    "    # Configure how to quantize weights.\n",
    "    def get_weights_and_quantizers(self, layer):\n",
    "      return [(layer.kernel, LastValueQuantizer(num_bits=8, symmetric=True, narrow_range=False, per_axis=False))]\n",
    "\n",
    "    # Configure how to quantize activations.\n",
    "    def get_activations_and_quantizers(self, layer):\n",
    "      return [(layer.activation, MovingAverageQuantizer(num_bits=8, symmetric=False, narrow_range=False, per_axis=False))]\n",
    "\n",
    "    def set_quantize_weights(self, layer, quantize_weights):\n",
    "      # Add this line for each item returned in `get_weights_and_quantizers`\n",
    "      # , in the same order\n",
    "      layer.kernel = quantize_weights[0]\n",
    "\n",
    "    def set_quantize_activations(self, layer, quantize_activations):\n",
    "      # Add this line for each item returned in `get_activations_and_quantizers`\n",
    "      # , in the same order.\n",
    "      layer.activation = quantize_activations[0]\n",
    "\n",
    "    # Configure how to quantize outputs (may be equivalent to activations).\n",
    "    def get_output_quantizers(self, layer):\n",
    "      return []\n",
    "\n",
    "    def get_config(self):\n",
    "      return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "quantize_layer_3 (QuantizeLa (None, 20)                3         \n",
      "_________________________________________________________________\n",
      "quant_custom_layer_1 (Quanti (None, 20)                425       \n",
      "_________________________________________________________________\n",
      "quant_flatten_16 (QuantizeWr (None, 20)                1         \n",
      "=================================================================\n",
      "Total params: 429\n",
      "Trainable params: 420\n",
      "Non-trainable params: 9\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "quantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer\n",
    "quantize_annotate_model = tfmot.quantization.keras.quantize_annotate_model\n",
    "quantize_scope = tfmot.quantization.keras.quantize_scope\n",
    "\n",
    "class CustomLayer(tf.keras.layers.Dense):\n",
    "  pass\n",
    "\n",
    "model = quantize_annotate_model(tf.keras.Sequential([\n",
    "   quantize_annotate_layer(CustomLayer(20, input_shape=(20,)), DefaultDenseQuantizeConfig()),\n",
    "   tf.keras.layers.Flatten()\n",
    "]))\n",
    "\n",
    "# `quantize_apply` requires mentioning `DefaultDenseQuantizeConfig` with `quantize_scope`\n",
    "# as well as the custom Keras layer.\n",
    "with quantize_scope(\n",
    "  {'DefaultDenseQuantizeConfig': DefaultDenseQuantizeConfig,\n",
    "   'CustomLayer': CustomLayer}):\n",
    "  # Use `quantize_apply` to actually make the model quantization aware.\n",
    "  quant_aware_model = tfmot.quantization.keras.quantize_apply(model)\n",
    "\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用自定义量化算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantize_annotate_layer = tfmot.quantization.keras.quantize_annotate_layer\n",
    "quantize_annotate_model = tfmot.quantization.keras.quantize_annotate_model\n",
    "quantize_scope = tfmot.quantization.keras.quantize_scope\n",
    "\n",
    "class FixedRangeQuantizer(tfmot.quantization.keras.quantizers.Quantizer):\n",
    "  \"\"\"Quantizer which forces outputs to be between -1 and 1.\"\"\"\n",
    "\n",
    "  def build(self, tensor_shape, name, layer):\n",
    "    # Not needed. No new TensorFlow variables needed.\n",
    "    return {}\n",
    "\n",
    "  def __call__(self, inputs, training, weights, **kwargs):\n",
    "    return tf.keras.backend.clip(inputs, -1.0, 1.0)\n",
    "\n",
    "  def get_config(self):\n",
    "    # Not needed. No __init__ parameters to serialize.\n",
    "    return {}\n",
    "\n",
    "\n",
    "class ModifiedDenseQuantizeConfig(DefaultDenseQuantizeConfig):\n",
    "    # Configure weights to quantize with 4-bit instead of 8-bits.\n",
    "    def get_weights_and_quantizers(self, layer):\n",
    "      # Use custom algorithm defined in `FixedRangeQuantizer` instead of default Quantizer.\n",
    "      return [(layer.kernel, FixedRangeQuantizer())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "quantize_layer_4 (QuantizeLa (None, 20)                3         \n",
      "_________________________________________________________________\n",
      "quant_dense_16 (QuantizeWrap (None, 20)                423       \n",
      "_________________________________________________________________\n",
      "quant_flatten_17 (QuantizeWr (None, 20)                1         \n",
      "=================================================================\n",
      "Total params: 427\n",
      "Trainable params: 420\n",
      "Non-trainable params: 7\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = quantize_annotate_model(tf.keras.Sequential([\n",
    "   # Pass in modified `QuantizeConfig` to modify this `Dense` layer.\n",
    "   quantize_annotate_layer(tf.keras.layers.Dense(20, input_shape=(20,)), ModifiedDenseQuantizeConfig()),\n",
    "   tf.keras.layers.Flatten()\n",
    "]))\n",
    "\n",
    "# `quantize_apply` requires mentioning `ModifiedDenseQuantizeConfig` with `quantize_scope`:\n",
    "with quantize_scope(\n",
    "  {'ModifiedDenseQuantizeConfig': ModifiedDenseQuantizeConfig}):\n",
    "  # Use `quantize_apply` to actually make the model quantization aware.\n",
    "  quant_aware_model = tfmot.quantization.keras.quantize_apply(model)\n",
    "\n",
    "quant_aware_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4c7c5c079346ce5c32a13af6f9c43b51d7027f08e22779c2ee30895e060b5a2d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 ('tf2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
